# å·¥å…·é›† (Tools)

æœ¬ç›®å½•åŒ…å«ä¸‰ä¸ªé«˜çº§è¯„ä¼°å’Œå¯è§†åŒ–å·¥å…·,ç”¨äºæ·±å…¥åˆ†æè®­ç»ƒå¥½çš„åˆ†ç±»æ¨¡å‹çš„æ€§èƒ½ã€‚

---

## ğŸ“‹ å·¥å…·åˆ—è¡¨

1. **visualize_gradcam.py** - GRAD-CAM++å¯è§£é‡Šæ€§çƒ­å›¾å¯è§†åŒ–å·¥å…· â­ **æ–°å¢**
2. **compare_models_auc.py** - åŒæ¨¡å‹å¤šæŒ‡æ ‡ç»Ÿè®¡æ¯”è¾ƒå·¥å…·
3. **evaluate_calibration.py** - æ¨¡å‹æ ¡å‡†æ€§èƒ½è¯„ä¼°å·¥å…·

---

# 0ï¸âƒ£ GRAD-CAM++å¯è§£é‡Šæ€§çƒ­å›¾å¯è§†åŒ–å·¥å…·

## ç®€ä»‹

`visualize_gradcam.py` æ˜¯ä¸€ä¸ªåŸºäºGRAD-CAM++ç®—æ³•çš„æ·±åº¦å­¦ä¹ æ¨¡å‹å¯è§£é‡Šæ€§åˆ†æå·¥å…·,ç”¨äºç”Ÿæˆå’Œå¯è§†åŒ–æ¨¡å‹å†³ç­–çš„çƒ­å›¾,å¸®åŠ©ç†è§£CNNæ¨¡å‹å…³æ³¨çš„å›¾åƒåŒºåŸŸã€‚

### æ ¸å¿ƒç‰¹æ€§

- **GRAD-CAM++ç®—æ³•**: ç›¸æ¯”æ ‡å‡†GRAD-CAMæ›´ç²¾ç¡®çš„æƒé‡è®¡ç®—,ç‰¹åˆ«é€‚åˆåŒ»å­¦å½±åƒåˆ†æ
- **æ™ºèƒ½å±‚æ£€æµ‹**: è‡ªåŠ¨æ£€æµ‹æœ€åä¸€ä¸ªå·ç§¯å±‚,æ”¯æŒ10+ç§ä¸»æµCNNæ¶æ„
- **Python API**: éå‘½ä»¤è¡Œå½¢å¼,ç›´æ¥åœ¨ä»£ç ä¸­è°ƒç”¨
- **å•å¼ +æ‰¹é‡**: æ”¯æŒå•å¼ å›¾ç‰‡å¤„ç†å’Œæ•´ä¸ªæ–‡ä»¶å¤¹æ‰¹é‡å¤„ç†
- **JETé¢œè‰²æ˜ å°„**: ç»å…¸çš„è“-é’-é»„-çº¢çƒ­å›¾é…è‰²
- **GPUåŠ é€Ÿ**: è‡ªåŠ¨GPU/CPUé€‚é…

### æ”¯æŒçš„æ¨¡å‹æ¶æ„

âœ… **æ”¯æŒçš„CNN**:
- InceptionResNetV2, ResNetç³»åˆ— (18/34/50/101/152)
- VGGç³»åˆ— (11/13/16åŠBNç‰ˆæœ¬), DenseNetç³»åˆ— (121/161/169/201)
- MobileNetV2, EfficientNetç³»åˆ— (B0-B7), ConvNeXtç³»åˆ—

âŒ **ä¸æ”¯æŒ**: Vision Transformer (ViT), Swin Transformer (éœ€è¦Attention Mapæ–¹æ³•)

---

## å¿«é€Ÿå¼€å§‹

### æ–¹æ³•1: ç›´æ¥è¿è¡Œè„šæœ¬

```bash
cd tools
python visualize_gradcam.py
```

è„šæœ¬ä¼šè‡ªåŠ¨å¤„ç†é¢„è®¾çš„æµ‹è¯•å›¾ç‰‡å¹¶ç”Ÿæˆçƒ­å›¾ã€‚

### æ–¹æ³•2: Python APIè°ƒç”¨ (æ¨è)

```python
from tools.visualize_gradcam import generate_gradcam

# å•å¼ å›¾ç‰‡å¤„ç†
result = generate_gradcam(
    image_path='datasets/test/1/sample.jpg',
    output_path='cam_output/sample_gradcam.jpg',
    alpha=0.5  # çƒ­å›¾é€æ˜åº¦
)

print(f"é¢„æµ‹: {result['pred_name']}, ç½®ä¿¡åº¦: {result['confidence']:.3f}")
```

### æ–¹æ³•3: æ‰¹é‡å¤„ç†

```python
from tools.visualize_gradcam import generate_gradcam_batch

# æ‰¹é‡å¤„ç†æ•´ä¸ªæ–‡ä»¶å¤¹
results = generate_gradcam_batch(
    image_dir='datasets/test/1/',
    output_dir='cam_output/batch_analysis',
    save_report=True  # ç”ŸæˆCSVæŠ¥å‘Š
)

print(f"å®Œæˆ! å…±å¤„ç†{len(results)}å¼ å›¾ç‰‡")
```

### æ–¹æ³•4: å¿«é€Ÿæ¨¡å¼

```python
from tools.visualize_gradcam import quick_gradcam

# ä½¿ç”¨é»˜è®¤é…ç½®å¿«é€Ÿç”Ÿæˆ
result = quick_gradcam('test.jpg', 'test_gradcam.jpg')
```

---

## å‡½æ•°å‚æ•°è¯´æ˜

### `generate_gradcam()` å‚æ•°åˆ—è¡¨

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|-----|------|--------|------|
| `image_path` | str | *å¿…éœ€* | è¾“å…¥å›¾ç‰‡è·¯å¾„ |
| `model_path` | str | `'models/inception_resnet_v2/...'` | æ¨¡å‹æƒé‡è·¯å¾„ |
| `backbone` | str | `'inception_resnet_v2'` | æ¨¡å‹æ¶æ„åç§° |
| `classes_path` | str | `'model_data/cls_classes.txt'` | ç±»åˆ«å®šä¹‰æ–‡ä»¶ |
| `input_shape` | tuple | `(299, 299)` | è¾“å…¥å°ºå¯¸ (H, W) |
| `target_class` | int | `None` | ç›®æ ‡ç±»åˆ«ç´¢å¼• (None=é¢„æµ‹ç±»åˆ«) |
| `alpha` | float | `0.5` | çƒ­å›¾é€æ˜åº¦ [0, 1] |
| `output_path` | str | `None` | è¾“å‡ºè·¯å¾„ (None=ä¸ä¿å­˜) |
| `cuda` | bool | `True` | æ˜¯å¦ä½¿ç”¨GPU |
| `return_image` | bool | `False` | æ˜¯å¦è¿”å›å›¾ç‰‡æ•°ç»„ |

### è¿”å›å€¼è¯´æ˜

è¿”å›ä¸€ä¸ªå­—å…¸,åŒ…å«ä»¥ä¸‹å­—æ®µ:

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `pred_class` | int | é¢„æµ‹ç±»åˆ«ç´¢å¼• |
| `pred_name` | str | é¢„æµ‹ç±»åˆ«åç§° |
| `confidence` | float | é¢„æµ‹ç½®ä¿¡åº¦ |
| `cam` | np.ndarray | çƒ­å›¾æ•°ç»„ [H, W] |
| `overlay` | np.ndarray | å åŠ å›¾ [H, W, 3] (å¦‚æœreturn_image=True) |
| `output_path` | str | ä¿å­˜è·¯å¾„ (å¦‚æœæŒ‡å®šäº†output_path) |

---

## è¾“å‡ºæ–‡ä»¶

### å•å¼ å¤„ç†è¾“å‡º

```
cam_output/
â””â”€â”€ sample_gradcam.jpg  # çƒ­å›¾å åŠ åŸå›¾ (JETé¢œè‰²æ˜ å°„)
```

### æ‰¹é‡å¤„ç†è¾“å‡º

```
cam_output/
â””â”€â”€ batch_analysis/
    â”œâ”€â”€ img001_gradcam.jpg
    â”œâ”€â”€ img002_gradcam.jpg
    â”œâ”€â”€ ...
    â””â”€â”€ gradcam_report.csv  # CSVæŠ¥å‘Š (å¯é€‰)
```

**CSVæŠ¥å‘Šæ ¼å¼**:
```csv
å›¾ç‰‡è·¯å¾„,é¢„æµ‹ç±»åˆ«,ç½®ä¿¡åº¦,çƒ­å›¾è·¯å¾„
datasets/test/1/img001.jpg,abnormal,0.9234,cam_output/batch_xxx/img001_gradcam.jpg
```

---

## ä½¿ç”¨æ¡ˆä¾‹

### æ¡ˆä¾‹1: åˆ†æå•å¼ å›¾ç‰‡çš„æ¨¡å‹å†³ç­–

```python
from tools.visualize_gradcam import generate_gradcam

# ç”Ÿæˆçƒ­å›¾ä»¥ç†è§£æ¨¡å‹å…³æ³¨åŒºåŸŸ
result = generate_gradcam(
    image_path='datasets/test/1/suspicious_case.jpg',
    output_path='analysis/case_gradcam.jpg',
    alpha=0.5
)

print(f"æ¨¡å‹é¢„æµ‹: {result['pred_name']}")
print(f"ç½®ä¿¡åº¦: {result['confidence']:.3f}")
# æ‰‹åŠ¨æŸ¥çœ‹ analysis/case_gradcam.jpg ç¡®è®¤æ¨¡å‹å…³æ³¨çš„åŒºåŸŸæ˜¯å¦åˆç†
```

### æ¡ˆä¾‹2: æ‰¹é‡åˆ†æé”™è¯¯åˆ†ç±»æ ·æœ¬

```python
from tools.visualize_gradcam import generate_gradcam_batch

# å¯¹è¯¯åˆ†ç±»æ ·æœ¬ç”Ÿæˆçƒ­å›¾,åˆ†æé”™è¯¯åŸå› 
results = generate_gradcam_batch(
    image_dir='datasets/misclassified/',
    output_dir='analysis/error_cases',
    save_report=True
)

print(f"å·²ç”Ÿæˆ{len(results)}ä¸ªé”™è¯¯æ¡ˆä¾‹çš„çƒ­å›¾")
# æŸ¥çœ‹çƒ­å›¾åˆ¤æ–­: æ¨¡å‹å…³æ³¨åŒºåŸŸæ˜¯å¦æ­£ç¡®? æ˜¯ç‰¹å¾æå–é—®é¢˜è¿˜æ˜¯æ•°æ®æ ‡æ³¨é—®é¢˜?
```

### æ¡ˆä¾‹3: å¯¹æ¯”ä¸åŒæ¨¡å‹çš„å…³æ³¨åŒºåŸŸ

```python
from tools.visualize_gradcam import generate_gradcam

image_path = 'datasets/test/1/sample.jpg'

# æ¨¡å‹A (InceptionResNetV2)
result_A = generate_gradcam(
    image_path=image_path,
    model_path='models/inception_resnet_v2/best_epoch_weights.pth',
    backbone='inception_resnet_v2',
    output_path='comparison/model_A_gradcam.jpg'
)

# æ¨¡å‹B (ResNet50)
result_B = generate_gradcam(
    image_path=image_path,
    model_path='models/resnet50/best_epoch_weights.pth',
    backbone='resnet50',
    input_shape=(224, 224),
    output_path='comparison/model_B_gradcam.jpg'
)

print(f"æ¨¡å‹Aå…³æ³¨åŒºåŸŸ vs æ¨¡å‹Bå…³æ³¨åŒºåŸŸ")
print(f"æ¨¡å‹Aç½®ä¿¡åº¦: {result_A['confidence']:.3f}")
print(f"æ¨¡å‹Bç½®ä¿¡åº¦: {result_B['confidence']:.3f}")
# å¯¹æ¯”ä¸¤å¼ çƒ­å›¾,åˆ¤æ–­å“ªä¸ªæ¨¡å‹çš„å…³æ³¨åŒºåŸŸæ›´åˆç†
```

---

## å¸¸è§é—®é¢˜ (FAQ)

### Q1: æç¤º"æ¨¡å‹ä¸æ”¯æŒGRAD-CAM"

**A**: æ‚¨ä½¿ç”¨çš„æ˜¯Transformeræ¶æ„(ViTã€Swin),ä¸æ”¯æŒGRAD-CAMã€‚
- **è§£å†³**: ä½¿ç”¨CNNæ¶æ„(ResNetã€InceptionResNetV2ç­‰)

---

### Q2: æç¤º"æ¨¡å‹ä¸­æœªæ‰¾åˆ°å±‚"

**A**: æ¨¡å‹ç›®æ ‡å±‚æ˜ å°„ä¸æ­£ç¡®ã€‚
- **è§£å†³æ–¹æ¡ˆ1**: è¿è¡Œ `python tools/print_model_structure.py --backbone ä½ çš„æ¨¡å‹å` æŸ¥çœ‹å±‚ç»“æ„
- **è§£å†³æ–¹æ¡ˆ2**: ç³»ç»Ÿä¼šè‡ªåŠ¨å°è¯•æ£€æµ‹æœ€åä¸€ä¸ªå·ç§¯å±‚

---

### Q3: çƒ­å›¾è´¨é‡ä¸ä½³æˆ–å®šä½ä¸å‡†

**A**: å¯èƒ½åŸå› å’Œä¼˜åŒ–:
1. **è°ƒæ•´é€æ˜åº¦**: `alpha=0.3` (é™ä½) æˆ– `alpha=0.7` (æé«˜)
2. **ç¡®ä¿æ¨¡å‹æ€§èƒ½**: å‡†ç¡®ç‡ä½çš„æ¨¡å‹çƒ­å›¾ä¹Ÿä¸å¯é 
3. **æ£€æŸ¥è¾“å…¥å›¾åƒ**: ç¡®ä¿å›¾åƒè´¨é‡è‰¯å¥½

---

### Q4: GPUå†…å­˜ä¸è¶³

**A**:
```python
# æ–¹æ¡ˆ1: ä½¿ç”¨CPU
result = generate_gradcam(..., cuda=False)

# æ–¹æ¡ˆ2: å•å¼ å¤„ç†è€Œéæ‰¹é‡
```

---

### Q5: å¦‚ä½•æŸ¥çœ‹æ¨¡å‹ç»“æ„?

**A**: ä½¿ç”¨è¾…åŠ©å·¥å…·:
```bash
python tools/print_model_structure.py --backbone inception_resnet_v2
```

è¾“å‡ºä¼šæ˜¾ç¤ºæ‰€æœ‰å±‚åç§°å’Œæ¨èçš„GRAD-CAMç›®æ ‡å±‚ã€‚

---

## æŠ€æœ¯ç»†èŠ‚

### GRAD-CAM++ vs GRAD-CAM

| ç‰¹æ€§ | GRAD-CAM | GRAD-CAM++ |
|-----|----------|-----------|
| æƒé‡è®¡ç®— | å…¨å±€å¹³å‡æ± åŒ–æ¢¯åº¦ | åŠ æƒæ¢¯åº¦(äºŒé˜¶å¯¼æ•°) |
| å¤šç›®æ ‡åœºæ™¯ | å¯èƒ½å®šä½ä¸å‡† | æ›´ç²¾ç¡®çš„å®šä½ |
| åŒ»å­¦å½±åƒé€‚ç”¨æ€§ | ä¸€èˆ¬ | ä¼˜ç§€ |
| è®¡ç®—å¤æ‚åº¦ | ä½ | ç¨é«˜(ä½†å¯æ¥å—) |

**æœ¬å·¥å…·é€‰æ‹©**: GRAD-CAM++(åŒæ—¶ä¹Ÿæä¾›æ ‡å‡†GRAD-CAMå®ç°ä½œä¸ºå¯¹æ¯”)

### æ ¸å¿ƒç®—æ³•

GRAD-CAM++æ”¹è¿›çš„æƒé‡è®¡ç®—å…¬å¼:
```
alpha = grad^2 / (2 * grad^2 + sum(A) * grad^3 + epsilon)
weights = sum(alpha * ReLU(grad))
cam = sum(weights * activations)
```

### é€‚ç”¨åœºæ™¯

âœ… **é€‚ç”¨**:
- ç†è§£CNNæ¨¡å‹çš„å†³ç­–ä¾æ®
- åˆ†æé”™è¯¯åˆ†ç±»æ¡ˆä¾‹
- è®ºæ–‡ä¸­å±•ç¤ºæ¨¡å‹å…³æ³¨åŒºåŸŸ
- åŒ»å­¦å½±åƒåˆ†æ(å®šä½ç—…ç¶)

âŒ **ä¸é€‚ç”¨**:
- Transformeræ¨¡å‹(æ— å·ç§¯å±‚)
- ä»…éœ€è¦åˆ†ç±»ç»“æœä¸éœ€è¦è§£é‡Š

---

## è¾…åŠ©å·¥å…·

### æ¨¡å‹ç»“æ„æŸ¥çœ‹å·¥å…·

```bash
# æŸ¥çœ‹ä»»æ„æ¨¡å‹çš„å±‚ç»“æ„
python tools/print_model_structure.py --backbone resnet50
python tools/print_model_structure.py --backbone efficientnet_b0
```

**è¾“å‡ºå†…å®¹**:
- é¡¶å±‚æ¨¡å—åˆ—è¡¨
- æ‰€æœ‰å±‚åç§°
- æ‰€æœ‰å·ç§¯å±‚
- æ¨èçš„GRAD-CAMç›®æ ‡å±‚

---

## æ›´æ–°æ—¥å¿—

### v1.0.0 (2025-11-28)
- âœ… åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- âœ… GRAD-CAM++æ ¸å¿ƒç®—æ³•å®ç°
- âœ… æ”¯æŒ10+ç§CNNæ¶æ„
- âœ… æ™ºèƒ½ç›®æ ‡å±‚è‡ªåŠ¨æ£€æµ‹
- âœ… Python APIæ¥å£(éå‘½ä»¤è¡Œ)
- âœ… å•å¼ +æ‰¹é‡å¤„ç†
- âœ… JETé¢œè‰²æ˜ å°„
- âœ… GPU/CPUè‡ªé€‚é…
- âœ… å®Œæ•´æ–‡æ¡£å’Œç¤ºä¾‹

---

## ç›¸å…³æ–‡æ¡£

- **å®Œæ•´APIæ–‡æ¡£**: `tools/cam/README.md`
- **å¿«é€Ÿä½¿ç”¨æŒ‡å—**: `GRADCAM_USAGE.md`
- **ä½¿ç”¨ç¤ºä¾‹è„šæœ¬**: `example_gradcam.py`

---

**Happy Visualizing! ğŸ”¥**

---

---



# 1ï¸âƒ£ åŒæ¨¡å‹å¤šæŒ‡æ ‡ç»Ÿè®¡æ¯”è¾ƒå·¥å…·

## ç®€ä»‹

`compare_models_auc.py` æ˜¯ä¸€ä¸ªåŸºäºé…å¯¹Bootstrapæ–¹æ³•çš„æ¨¡å‹æ€§èƒ½ç»Ÿè®¡æ¯”è¾ƒå·¥å…·,ç”¨äºç§‘å­¦ä¸¥è°¨åœ°æ¯”è¾ƒä¸¤ä¸ªæ·±åº¦å­¦ä¹ åˆ†ç±»æ¨¡å‹çš„æ•´ä½“æ€§èƒ½å·®å¼‚ã€‚

### æ ¸å¿ƒç‰¹æ€§

- **ç»Ÿè®¡ä¸¥è°¨**: ä½¿ç”¨é…å¯¹Bootstrapæ–¹æ³•è®¡ç®—ç½®ä¿¡åŒºé—´å’Œpå€¼,é¿å…ç®€å•æ¯”è¾ƒçš„è¯¯å¯¼æ€§
- **å¤šæŒ‡æ ‡æ”¯æŒ**: æ”¯æŒ6ç§æ•´ä½“è¯„ä¼°æŒ‡æ ‡(Macro/Micro AUC, Accuracy, Precision, Recall, F1)
- **æ•´ä½“æ€§èƒ½**: æ‰€æœ‰æŒ‡æ ‡å‡ä¸ºæ¨¡å‹æ•´ä½“è¯„ä¼°,é€‚ç”¨äº2åˆ†ç±»å’ŒNåˆ†ç±»åœºæ™¯
- **ä¸“ä¸šå¯è§†åŒ–**: ç”Ÿæˆé«˜è´¨é‡çš„å·®å¼‚åˆ†å¸ƒå›¾å’Œå¯¹æ¯”æŸ±çŠ¶å›¾
- **è¯¦ç»†æŠ¥å‘Š**: è¾“å‡ºå®Œæ•´çš„ç»Ÿè®¡åˆ†ææ–‡æœ¬æŠ¥å‘Š

### æ”¯æŒçš„æŒ‡æ ‡

| æŒ‡æ ‡åç§° | è¯´æ˜ | é€‚ç”¨åœºæ™¯ |
|---------|------|---------|
| `macro_auc` | Macro-averaged AUC (OvR) | æ•´ä½“æ¨¡å‹æ€§èƒ½(ç±»åˆ«å¹³è¡¡) |
| `micro_auc` | Micro-averaged AUC (OvR) | æ•´ä½“æ¨¡å‹æ€§èƒ½(æ ·æœ¬çº§) |
| `accuracy` | æ•´ä½“å‡†ç¡®ç‡ | æ‰€æœ‰ç±»åˆ«ç»¼åˆå‡†ç¡®æ€§ |
| `macro_precision` | Macro-averaged Precision | æ•´ä½“ç²¾ç¡®åº¦(ç±»åˆ«å¹³è¡¡) |
| `macro_recall` | Macro-averaged Recall (Sensitivity) | æ•´ä½“å¬å›ç‡(ç±»åˆ«å¹³è¡¡) |
| `macro_f1` | Macro-averaged F1-score | ç²¾ç¡®åº¦å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡ |

---

## å¿«é€Ÿå¼€å§‹

### å‰ææ¡ä»¶

1. å·²è¿è¡Œ `eval.py` ç”Ÿæˆä¸¤ä¸ªæ¨¡å‹çš„ `detailed_predictions.csv` æ–‡ä»¶
2. ä¸¤ä¸ªCSVæ–‡ä»¶å¿…é¡»åœ¨**å®Œå…¨ç›¸åŒçš„æµ‹è¯•é›†**ä¸Šè¯„ä¼°(ç›¸åŒæ ·æœ¬é¡ºåºå’ŒçœŸå®æ ‡ç­¾)

### ä½¿ç”¨æ–¹æ³•: Pythonå‡½æ•°è°ƒç”¨

```python
from tools.compare_models_auc import compare_two_models

# ç¤ºä¾‹1: åŸºç¡€ç”¨æ³• - æ¯”è¾ƒMacro AUC
results = compare_two_models(
    'metrics_out/inception_resnet_v2_cls_test/detailed_predictions.csv',
    'metrics_out/resnet50_cls_test/detailed_predictions.csv',
    model_name1='InceptionResNetV2',
    model_name2='ResNet50'
)

# ç¤ºä¾‹2: å¤šæŒ‡æ ‡æ¯”è¾ƒ
results = compare_two_models(
    'metrics_out/model_A/detailed_predictions.csv',
    'metrics_out/model_B/detailed_predictions.csv',
    model_name1='Model A',
    model_name2='Model B',
    metrics=['macro_auc', 'micro_auc', 'accuracy', 'macro_f1'],
    n_bootstrap=2000,
    ci_level=99
)

# ç¤ºä¾‹3: é™é»˜æ¨¡å¼(ä¸æ‰“å°è¯¦ç»†ä¿¡æ¯)
results = compare_two_models(
    'metrics_out/model_A/detailed_predictions.csv',
    'metrics_out/model_B/detailed_predictions.csv',
    verbose=False  # é™é»˜è¿è¡Œ,é€‚åˆæ‰¹é‡å¤„ç†
)

# ç¤ºä¾‹4: è®¿é—®ç»“æœ
print(f"Macro AUCå·®å¼‚: {results['macro_auc']['diff_original']:.4f}")
print(f"95% CI: [{results['macro_auc']['ci_lower']:.4f}, {results['macro_auc']['ci_upper']:.4f}]")
print(f"på€¼: {results['macro_auc']['p_value']:.4f}")
print(f"æ˜¯å¦æ˜¾è‘—: {results['macro_auc']['significant']}")
print(f"æ•ˆåº”é‡: {results['macro_auc']['effect_size']}")
```

---

## å‡½æ•°å‚æ•°è¯´æ˜

### `compare_two_models()` å‚æ•°åˆ—è¡¨

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|-----|------|--------|------|
| `csv_path1` | str | *å¿…éœ€* | æ¨¡å‹1çš„CSVæ–‡ä»¶è·¯å¾„ |
| `csv_path2` | str | *å¿…éœ€* | æ¨¡å‹2çš„CSVæ–‡ä»¶è·¯å¾„ |
| `model_name1` | str | `'Model_A'` | æ¨¡å‹1çš„æ˜¾ç¤ºåç§° |
| `model_name2` | str | `'Model_B'` | æ¨¡å‹2çš„æ˜¾ç¤ºåç§° |
| `output_dir` | str | `'metrics_out/model_comparison'` | è¾“å‡ºç›®å½•è·¯å¾„ |
| `n_bootstrap` | int | `1000` | Bootstrapé‡é‡‡æ ·æ¬¡æ•°(å»ºè®®1000-5000) |
| `ci_level` | float | `95.0` | ç½®ä¿¡æ°´å¹³,ç™¾åˆ†æ¯”(å¸¸ç”¨: 90, 95, 99) |
| `metrics` | list | `['macro_auc']` | æŒ‡æ ‡åˆ—è¡¨ |
| `random_state` | int | `42` | éšæœºç§å­(ç”¨äºç»“æœå¯å¤ç°) |
| `verbose` | bool | `True` | æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯ |

### è¿”å›å€¼è¯´æ˜

è¿”å›ä¸€ä¸ªå­—å…¸,é”®ä¸ºæŒ‡æ ‡åç§°,å€¼ä¸ºåŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸:

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `metric1_original` | float | æ¨¡å‹1çš„åŸå§‹æŒ‡æ ‡å€¼ |
| `metric2_original` | float | æ¨¡å‹2çš„åŸå§‹æŒ‡æ ‡å€¼ |
| `diff_original` | float | å·®å¼‚å€¼(æ¨¡å‹1 - æ¨¡å‹2) |
| `diff_bootstrap` | np.ndarray | Bootstrapå·®å¼‚åˆ†å¸ƒæ•°ç»„ |
| `diff_mean` | float | Bootstrapå·®å¼‚å‡å€¼ |
| `diff_std` | float | Bootstrapå·®å¼‚æ ‡å‡†å·® |
| `ci_lower` | float | ç½®ä¿¡åŒºé—´ä¸‹ç•Œ |
| `ci_upper` | float | ç½®ä¿¡åŒºé—´ä¸Šç•Œ |
| `p_value` | float | åŒä¾§på€¼ |
| `significant` | bool | æ˜¯å¦å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§ |
| `effect_size` | str | æ•ˆåº”é‡("Negligible", "Small", "Medium", "Large") |

---

## è¾“å‡ºæ–‡ä»¶

è¿è¡Œå®Œæˆå,åœ¨ `metrics_out/model_comparison/` ç›®å½•ä¸‹ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶:

### 1. å¯è§†åŒ–æ–‡ä»¶

#### `metrics_difference_distribution.png`
- **å†…å®¹**: å¤šæŒ‡æ ‡å·®å¼‚çš„Bootstrapåˆ†å¸ƒç›´æ–¹å›¾
- **å¸ƒå±€**: æ ¹æ®æŒ‡æ ‡æ•°é‡è‡ªåŠ¨è°ƒæ•´(1ä¸ª: å•å›¾, 2ä¸ª: 1x2, 3-4ä¸ª: 2x2, 5-6ä¸ª: 2x3)
- **æ ‡æ³¨**:
  - çº¢è‰²è™šçº¿: 95% CIä¸Šä¸‹ç•Œ
  - ç»¿è‰²å®çº¿: 0çº¿(æ— å·®å¼‚åŸºå‡†)
  - æ ‡é¢˜æ˜¾ç¤ºæŒ‡æ ‡åå’Œpå€¼
  - `*`æ ‡è®°è¡¨ç¤ºç»Ÿè®¡æ˜¾è‘—æ€§

#### `metrics_comparison_barplot.png`
- **å†…å®¹**: åŒæ¨¡å‹æŒ‡æ ‡å€¼å¯¹æ¯”æŸ±çŠ¶å›¾
- **ç‰¹ç‚¹**:
  - åˆ†ç»„æŸ±çŠ¶å›¾(æ¯ç»„ä¸¤æ ¹æŸ±å­)
  - æŸ±é¡¶æ ‡æ³¨å…·ä½“æ•°å€¼
  - æ˜¾è‘—å·®å¼‚æŒ‡æ ‡é¡¶éƒ¨æ ‡æ³¨ `*`

### 2. æ–‡æœ¬æŠ¥å‘Š

#### `comparison_report.txt`
- **å†…å®¹**: å®Œæ•´çš„ç»Ÿè®¡åˆ†ææŠ¥å‘Š
- **åŒ…å«**:
  - å®éªŒé…ç½®ä¿¡æ¯
  - æ¯ä¸ªæŒ‡æ ‡çš„åŸå§‹å€¼ã€å·®å¼‚ã€ç½®ä¿¡åŒºé—´
  - på€¼å’Œç»Ÿè®¡æ˜¾è‘—æ€§ç»“è®º
  - æ•ˆåº”é‡è¯„ä¼°
  - æ€»ä½“ç»“è®ºå’Œè§£è¯»è¯´æ˜

---

## è¾“å‡ºè§£è¯»

### æ§åˆ¶å°è¾“å‡ºç¤ºä¾‹

```
========================================
æ¯”è¾ƒç»“æœæ‘˜è¦
========================================
Macro Auc      : +0.0289 [95% CI: +0.0051, +0.0527] **  (p=0.017)
Accuracy       : +0.0200 [95% CI: -0.0023, +0.0423]     (p=0.078)
Macro F1       : +0.0200 [95% CI: +0.0001, +0.0399] *   (p=0.049)

æ˜¾è‘—æ€§æ ‡è®°: *** p<0.01, ** p<0.05, * p<0.1

ç»“è®º: Model A åœ¨ Macro Auc, Macro F1 ä¸Šæ˜¾è‘—ä¼˜äºå¯¹æ¯”æ¨¡å‹
========================================
```

### å…³é”®æŒ‡æ ‡è§£è¯»

1. **å·®å¼‚å€¼** (å¦‚ `+0.0289`)
   - æ­£å€¼: æ¨¡å‹1ä¼˜äºæ¨¡å‹2
   - è´Ÿå€¼: æ¨¡å‹2ä¼˜äºæ¨¡å‹1
   - æ•°å€¼å¤§å°: æ€§èƒ½å·®è·

2. **95% ç½®ä¿¡åŒºé—´** (å¦‚ `[+0.0051, +0.0527]`)
   - **ä¸åŒ…å«0**: å·®å¼‚å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§
   - **åŒ…å«0**: å·®å¼‚æ— ç»Ÿè®¡æ˜¾è‘—æ€§
   - åŒºé—´å®½åº¦: ä¼°è®¡ä¸ç¡®å®šæ€§(çª„=æ›´å¯é )

3. **på€¼** (å¦‚ `p=0.017`)
   - `p < 0.01`: é«˜åº¦æ˜¾è‘— (`***`)
   - `p < 0.05`: æ˜¾è‘— (`**`)
   - `p < 0.1`: è¾¹ç¼˜æ˜¾è‘— (`*`)
   - `p â‰¥ 0.1`: æ— æ˜¾è‘—å·®å¼‚

4. **æ•ˆåº”é‡** (æŠ¥å‘Šä¸­)
   - Negligible: |å·®å¼‚| < 0.02 (å¯å¿½ç•¥)
   - Small: 0.02 â‰¤ |å·®å¼‚| < 0.05 (å°)
   - Medium: 0.05 â‰¤ |å·®å¼‚| < 0.10 (ä¸­ç­‰)
   - Large: |å·®å¼‚| â‰¥ 0.10 (å¤§)

---

## ä½¿ç”¨æ¡ˆä¾‹

### æ¡ˆä¾‹1: æ¯”è¾ƒä¸¤ä¸ªé¢„è®­ç»ƒæ¨¡å‹

```python
# åœºæ™¯: æ¯”è¾ƒInceptionResNetV2å’ŒResNet50åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½
from tools.compare_models_auc import compare_two_models

results = compare_two_models(
    'metrics_out/inception_resnet_v2_cls_test/detailed_predictions.csv',
    'metrics_out/resnet50_cls_test/detailed_predictions.csv',
    model_name1='InceptionResNetV2',
    model_name2='ResNet50',
    metrics=['macro_auc', 'accuracy', 'macro_recall'],
    n_bootstrap=2000
)
```

**é¢„æœŸè¾“å‡º**: åˆ¤æ–­å“ªä¸ªæ¨¡å‹æ›´é€‚åˆè¯¥ä»»åŠ¡

### æ¡ˆä¾‹2: è¯„ä¼°æ•°æ®å¢å¼ºçš„æ•ˆæœ

```python
# åœºæ™¯: æ¯”è¾ƒä½¿ç”¨/ä¸ä½¿ç”¨æ•°æ®å¢å¼ºè®­ç»ƒçš„ç›¸åŒæ¨¡å‹
from tools.compare_models_auc import compare_two_models

results = compare_two_models(
    'metrics_out/model_with_augmentation/detailed_predictions.csv',
    'metrics_out/model_without_augmentation/detailed_predictions.csv',
    model_name1='With Augmentation',
    model_name2='Without Augmentation',
    metrics=['macro_auc', 'macro_precision', 'macro_recall']
)
```

**é¢„æœŸè¾“å‡º**: é‡åŒ–æ•°æ®å¢å¼ºçš„æ€§èƒ½æå‡

### æ¡ˆä¾‹3: å¯¹æ¯”ä¸åŒæŸå¤±å‡½æ•°

```python
# åœºæ™¯: æ¯”è¾ƒä½¿ç”¨Focal Loss vs Cross Entropyè®­ç»ƒçš„æ¨¡å‹
from tools.compare_models_auc import compare_two_models

results = compare_two_models(
    'metrics_out/model_focal_loss/detailed_predictions.csv',
    'metrics_out/model_cross_entropy/detailed_predictions.csv',
    model_name1='Focal Loss',
    model_name2='Cross Entropy',
    metrics=['macro_auc', 'micro_auc', 'accuracy', 'macro_f1'],
    ci_level=99
)
```

**é¢„æœŸè¾“å‡º**: éªŒè¯Focal Lossåœ¨ç±»åˆ«ä¸å¹³è¡¡æ•°æ®ä¸Šçš„ä¼˜åŠ¿

---

## å¸¸è§é—®é¢˜ (FAQ)

### Q1: ä¸ºä»€ä¹ˆä¸¤ä¸ªCSVçš„çœŸå®æ ‡ç­¾å¿…é¡»å®Œå…¨ç›¸åŒ?

**A**: é…å¯¹Bootstrapæ–¹æ³•è¦æ±‚å¯¹ç›¸åŒæ ·æœ¬è¿›è¡Œé‡é‡‡æ ·,ç¡®ä¿æ¯”è¾ƒçš„å…¬å¹³æ€§ã€‚å¦‚æœæµ‹è¯•é›†ä¸åŒ,ç»“è®ºå¯èƒ½å—æ•°æ®é›†å·®å¼‚å½±å“è€Œéæ¨¡å‹æ€§èƒ½å·®å¼‚ã€‚

---

### Q2: Bootstrapæ¬¡æ•°é€‰æ‹©å¤šå°‘åˆé€‚?

**A**:
- **é»˜è®¤1000æ¬¡**: å¹³è¡¡é€Ÿåº¦å’Œç²¾åº¦,é€‚åˆå¤§å¤šæ•°åœºæ™¯
- **2000-5000æ¬¡**: è¿½æ±‚æ›´é«˜ç²¾åº¦,æ ·æœ¬é‡è¾ƒå°æ—¶æ¨è
- **500æ¬¡**: å¿«é€Ÿæµ‹è¯•(ä¸æ¨èç”¨äºæœ€ç»ˆç»“è®º)

---

### Q3: å¦‚ä½•åˆ¤æ–­å·®å¼‚æ˜¯å¦æ˜¾è‘—?

**A**: ä¸‰ç§æ–¹æ³•(æ¨èä½¿ç”¨å‰ä¸¤ç§):
1. **ç½®ä¿¡åŒºé—´æ³•** (ä¸»è¦): CIä¸åŒ…å«0 â†’ æ˜¾è‘—
2. **på€¼æ³•** (è¾…åŠ©): p < 0.05 â†’ æ˜¾è‘—
3. **æ•ˆåº”é‡** (è¡¥å……): å³ä½¿æ˜¾è‘—,æ•ˆåº”é‡å°å¯èƒ½å®é™…æ„ä¹‰æœ‰é™

---

### Q4: æŠ¥é”™ "çœŸå®æ ‡ç­¾ä¸ä¸€è‡´" æ€ä¹ˆåŠ?

**A**: å¯èƒ½åŸå› :
1. ä¸¤ä¸ªæ¨¡å‹åœ¨ä¸åŒæµ‹è¯•é›†ä¸Šè¯„ä¼° â†’ **ç¡®ä¿ä½¿ç”¨ç›¸åŒçš„ `cls_test.txt`**
2. CSVæ–‡ä»¶çš„æ ·æœ¬é¡ºåºä¸åŒ â†’ **é‡æ–°è¿è¡Œ `eval.py` ç”ŸæˆCSV**
3. ä¸€ä¸ªCSVæ˜¯è®­ç»ƒé›†,ä¸€ä¸ªæ˜¯æµ‹è¯•é›† â†’ **æ£€æŸ¥æ–‡ä»¶è·¯å¾„**

---

### Q5: æŠ¥é”™ "æ— æ³•è¯†åˆ«æ¦‚ç‡åˆ—" æ€ä¹ˆåŠ?

**A**: æ£€æŸ¥CSVæ–‡ä»¶æ˜¯å¦åŒ…å«æ¦‚ç‡åˆ—:
- **æ­£ç¡®æ ¼å¼1**: `normal_probability`, `abnormal_probability`
- **æ­£ç¡®æ ¼å¼2**: `class_0_prob`, `class_1_prob`
- **é”™è¯¯æ ¼å¼**: `prob_0`, `probability_normal` (ä¸æ”¯æŒ)

å¦‚æœæ ¼å¼é”™è¯¯,éœ€è¦ä¿®æ”¹ `eval.py` é‡æ–°ç”ŸæˆCSVã€‚

---

### Q6: èƒ½å¦æ¯”è¾ƒ3ä¸ªä»¥ä¸Šçš„æ¨¡å‹?

**A**: å½“å‰ç‰ˆæœ¬ä»…æ”¯æŒä¸¤ä¸¤æ¯”è¾ƒã€‚å¦‚éœ€æ¯”è¾ƒå¤šä¸ªæ¨¡å‹:
1. ä¸¤ä¸¤è¿è¡Œæœ¬å·¥å…·(å¦‚A vs B, A vs C, B vs C)
2. æ±‡æ€»ç»“æœè¿›è¡Œç»¼åˆåˆ†æ

æœªæ¥ç‰ˆæœ¬å¯èƒ½æ”¯æŒå¤šæ¨¡å‹æ‰¹é‡æ¯”è¾ƒã€‚

---

### Q7: Macro AUCå’ŒMicro AUCæœ‰ä»€ä¹ˆåŒºåˆ«?

**A**:
- **Macro AUC**: æ¯ä¸ªç±»åˆ«AUCçš„ç®—æœ¯å¹³å‡,**ç±»åˆ«å¹³ç­‰æƒé‡**,é€‚åˆç±»åˆ«å¹³è¡¡åœºæ™¯
- **Micro AUC**: æ±‡æ€»æ‰€æœ‰æ ·æœ¬è®¡ç®—å…¨å±€AUC,**æ ·æœ¬çº§æƒé‡**,å¤§ç±»åˆ«å½±å“æ›´å¤§

**æ¨è**: ç±»åˆ«å¹³è¡¡æ—¶ç”¨Macro, ç±»åˆ«ä¸å¹³è¡¡æ—¶åŒæ—¶çœ‹Macroå’ŒMicro

---

### Q8: è¿è¡Œé€Ÿåº¦æ…¢æ€ä¹ˆåŠ?

**A**: ä¼˜åŒ–å»ºè®®:
1. å‡å°‘Bootstrapæ¬¡æ•°(å¦‚ä»2000é™åˆ°1000)
2. å‡å°‘æ¯”è¾ƒæŒ‡æ ‡æ•°é‡(å…ˆæ¯”è¾ƒæ ¸å¿ƒæŒ‡æ ‡)
3. æœªæ¥ç‰ˆæœ¬å¯èƒ½æ”¯æŒå¤šæ ¸å¹¶è¡ŒåŠ é€Ÿ

**å‚è€ƒé€Ÿåº¦**: 1000æ¬¡Bootstrap Ã— 3æŒ‡æ ‡ Ã— 500æ ·æœ¬ â‰ˆ 5-10ç§’

---

### Q9: å¯è§†åŒ–ä¸­æ–‡æ˜¾ç¤ºä¸ºæ–¹å—æ€ä¹ˆåŠ?

**A**: å®‰è£…ä¸­æ–‡å­—ä½“:
```bash
# Windows: ç³»ç»Ÿè‡ªå¸¦SimHei,ä¸€èˆ¬æ— é—®é¢˜
# Linux: å®‰è£…å­—ä½“
sudo apt-get install fonts-wqy-microhei
```

æˆ–ä¿®æ”¹è„šæœ¬ç¬¬356è¡Œ,ä½¿ç”¨ç³»ç»Ÿå­—ä½“:
```python
plt.rcParams['font.sans-serif'] = ['Arial']  # ä½¿ç”¨è‹±æ–‡å­—ä½“
```

---

### Q10: èƒ½å¦è‡ªå®šä¹‰è¾“å‡ºç›®å½•?

**A**: å¯ä»¥,ä½¿ç”¨ `output_dir` å‚æ•°:
```python
from tools.compare_models_auc import compare_two_models

results = compare_two_models(
    'metrics_out/model_A/detailed_predictions.csv',
    'metrics_out/model_B/detailed_predictions.csv',
    output_dir='my_results/comparison_2025_11_28'
)
```

---

## æŠ€æœ¯ç»†èŠ‚

### Bootstrapæ–¹æ³•è¯´æ˜

æœ¬å·¥å…·ä½¿ç”¨**é…å¯¹åˆ†å±‚Bootstrap**:

1. **é…å¯¹è®¾è®¡**: ä¸¤æ¨¡å‹åœ¨æ¯æ¬¡é‡é‡‡æ ·ä¸­ä½¿ç”¨ç›¸åŒçš„æ ·æœ¬ç´¢å¼•,æ¶ˆé™¤æ•°æ®é›†éšæœºæ€§çš„å½±å“
2. **åˆ†å±‚æŠ½æ ·**: ä¿æŒç±»åˆ«åˆ†å¸ƒä¸åŸå§‹æ•°æ®ä¸€è‡´,é€‚åˆä¸å¹³è¡¡æ•°æ®é›†
3. **ç™¾åˆ†ä½æ³•**: ä½¿ç”¨Bootstrapå·®å¼‚åˆ†å¸ƒçš„ç™¾åˆ†ä½æ•°è®¡ç®—ç½®ä¿¡åŒºé—´
4. **åŒä¾§æ£€éªŒ**: på€¼è®¡ç®—è€ƒè™‘åŒå‘å·®å¼‚,é€‚åˆ"æ˜¯å¦å­˜åœ¨å·®å¼‚"çš„å‡è®¾æ£€éªŒ

### ç»Ÿè®¡å‡è®¾

- **é›¶å‡è®¾(H0)**: ä¸¤æ¨¡å‹æ€§èƒ½æ— å·®å¼‚(å·®å¼‚=0)
- **å¤‡æ‹©å‡è®¾(H1)**: ä¸¤æ¨¡å‹æ€§èƒ½æœ‰å·®å¼‚(å·®å¼‚â‰ 0)
- **æ˜¾è‘—æ€§æ°´å¹³**: Î± = 1 - ci_level / 100 (é»˜è®¤0.05)

### äºŒåˆ†ç±»AUCè®¡ç®—ç»†èŠ‚

æœ¬å·¥å…·å¯¹**äºŒåˆ†ç±»**å’Œ**å¤šåˆ†ç±»**åœºæ™¯é‡‡ç”¨ä¸åŒçš„AUCè®¡ç®—ç­–ç•¥:

#### äºŒåˆ†ç±» (n_classes = 2)

- **Macro AUC**: æ‰‹åŠ¨æ„é€ One-Hotç¼–ç çŸ©é˜µ,åˆ†åˆ«è®¡ç®—æ¯ä¸ªç±»åˆ«çš„AUCåå–å¹³å‡
  ```python
  labels_bin = np.zeros((len(labels), 2), dtype=int)
  labels_bin[np.arange(len(labels)), labels] = 1
  auc_0 = roc_auc_score(labels_bin[:, 0], probs[:, 0])
  auc_1 = roc_auc_score(labels_bin[:, 1], probs[:, 1])
  macro_auc = (auc_0 + auc_1) / 2
  ```
- **Micro AUC**: ç›´æ¥ä½¿ç”¨æ­£ç±»(ç±»åˆ«1)çš„æ¦‚ç‡è®¡ç®—
  ```python
  micro_auc = roc_auc_score(labels, probs[:, 1])
  ```

**è®¾è®¡åŸå› **: sklearnçš„`label_binarize`åœ¨äºŒåˆ†ç±»æ—¶åªè¿”å›å½¢çŠ¶ä¸º`(n_samples, 1)`çš„æ•°ç»„,æ— æ³•æ»¡è¶³OvRç­–ç•¥éœ€è¦çš„å®Œæ•´äºŒå€¼åŒ–çŸ©é˜µã€‚

#### å¤šåˆ†ç±» (n_classes â‰¥ 3)

- **Macro AUC**: ç›´æ¥ä½¿ç”¨sklearnçš„One-vs-Restç­–ç•¥
  ```python
  macro_auc = roc_auc_score(labels, probs, average='macro', multi_class='ovr')
  ```
- **Micro AUC**: åŒæ ·ä½¿ç”¨sklearnçš„OvRç­–ç•¥
  ```python
  micro_auc = roc_auc_score(labels, probs, average='micro', multi_class='ovr')
  ```

### é€‚ç”¨åœºæ™¯

âœ… **é€‚ç”¨**:
- æ¯”è¾ƒä¸åŒæ¨¡å‹æ¶æ„çš„æ€§èƒ½
- è¯„ä¼°è®­ç»ƒç­–ç•¥(æ•°æ®å¢å¼ºã€æŸå¤±å‡½æ•°ç­‰)çš„æ•ˆæœ
- éªŒè¯æ¨¡å‹æ”¹è¿›æ˜¯å¦æœ‰ç»Ÿè®¡å­¦æ„ä¹‰

âŒ **ä¸é€‚ç”¨**:
- æ ·æœ¬é‡è¿‡å°(<30æ ·æœ¬,Bootstrapä¸ç¨³å®š)
- æµ‹è¯•é›†ä¸åŒ(è¿åé…å¯¹è®¾è®¡å‰æ)
- è®­ç»ƒé›†/éªŒè¯é›†æ¯”è¾ƒ(åº”ä½¿ç”¨ç‹¬ç«‹æµ‹è¯•é›†)

---

## å¼•ç”¨

å¦‚æœæœ¬å·¥å…·å¯¹æ‚¨çš„ç ”ç©¶æœ‰å¸®åŠ©,è¯·åœ¨è®ºæ–‡ä¸­å¼•ç”¨Bootstrapæ–¹æ³•:

```
Efron, B., & Tibshirani, R. J. (1994).
An introduction to the bootstrap.
CRC press.
```

---

## æ›´æ–°æ—¥å¿—

### v1.0.1 (2025-11-28)
- ğŸ› ä¿®å¤äºŒåˆ†ç±»åœºæ™¯ä¸‹Macro AUCè®¡ç®—çš„IndexError
- ğŸ”§ ä½¿ç”¨æ‰‹åŠ¨One-Hotç¼–ç æ›¿ä»£sklearnçš„label_binarize
- âœ… ç¡®ä¿äºŒåˆ†ç±»å’Œå¤šåˆ†ç±»åœºæ™¯çš„å…¼å®¹æ€§

### v1.0.0 (2025-11-28)
- âœ… åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- âœ… æ”¯æŒ6ç§æ•´ä½“è¯„ä¼°æŒ‡æ ‡(macro_auc, micro_auc, accuracy, macro_precision, macro_recall, macro_f1)
- âœ… é…å¯¹Bootstrapç»Ÿè®¡æ£€éªŒ
- âœ… ä¸“ä¸šå¯è§†åŒ–å’ŒæŠ¥å‘Šç”Ÿæˆ
- âœ… å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œç”¨æˆ·æç¤º
- âœ… çº¯Pythonå‡½æ•°æ¥å£,æ— å‘½ä»¤è¡Œä¾èµ–

---

## è”ç³»ä¸åé¦ˆ

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®,è¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»:
- é¡¹ç›®ä»“åº“: [GitHub Issues]
- é‚®ç®±: [è”ç³»é‚®ç®±]

---

**Happy Comparing! ğŸš€**

---
---

# 2ï¸âƒ£ æ¨¡å‹æ ¡å‡†æ€§èƒ½è¯„ä¼°å·¥å…·

## ç®€ä»‹

`evaluate_calibration.py` æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°åˆ†ç±»æ¨¡å‹æ¦‚ç‡é¢„æµ‹å¯é æ€§çš„å·¥å…·ã€‚é€šè¿‡è®¡ç®—Calibration Plot(æ ¡å‡†æ›²çº¿)å’ŒBrier Score,å¸®åŠ©ç”¨æˆ·åˆ¤æ–­æ¨¡å‹è¾“å‡ºçš„æ¦‚ç‡å€¼æ˜¯å¦çœŸå®åæ˜ é¢„æµ‹çš„ç½®ä¿¡åº¦ã€‚

### æ ¸å¿ƒç‰¹æ€§

- **Calibration Plot (å¯é æ€§æ›²çº¿)**: å¯è§†åŒ–é¢„æµ‹æ¦‚ç‡ä¸å®é™…æ­£ç¡®ç‡çš„å¯¹åº”å…³ç³»
- **Brier Score**: é‡åŒ–æ¦‚ç‡é¢„æµ‹çš„å‡†ç¡®æ€§(è¶Šå°è¶Šå¥½,0ä¸ºå®Œç¾)
- **æ•´ä½“+å„ç±»åˆ«æ ¡å‡†**: åŒæ—¶æä¾›æ¨¡å‹æ•´ä½“æ ¡å‡†å’Œå„ç±»åˆ«ç‹¬ç«‹æ ¡å‡†åˆ†æ
- **sklearn API**: åŸºäº`sklearn.calibration.calibration_curve`å®ç°,ç»“æœå¯é 
- **ä¸“ä¸šå¯è§†åŒ–**: ä½¿ç”¨Times New Romanå­—ä½“,è‹±æ–‡è¾“å‡º,é€‚åˆå­¦æœ¯è®ºæ–‡

### ä¸ºä»€ä¹ˆéœ€è¦æ ¡å‡†è¯„ä¼°?

**åœºæ™¯ç¤ºä¾‹**:
- æ¨¡å‹Aé¢„æµ‹æŸæ ·æœ¬ä¸ºç±»åˆ«1çš„æ¦‚ç‡æ˜¯80%,ä½†å®é™…å‡†ç¡®ç‡åªæœ‰60% â†’ **æ ¡å‡†å·®**
- æ¨¡å‹Bé¢„æµ‹æ¦‚ç‡80%,å®é™…å‡†ç¡®ç‡ä¹Ÿæ˜¯80% â†’ **æ ¡å‡†å¥½**

**åº”ç”¨ä»·å€¼**:
- åŒ»å­¦è¯Šæ–­: ç½®ä¿¡åº¦å†³å®šæ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥
- é£é™©è¯„ä¼°: æ¦‚ç‡å€¼ç›´æ¥ç”¨äºå†³ç­–é˜ˆå€¼
- æ¨¡å‹é€‰æ‹©: AUCç›¸ä¼¼æ—¶,æ ¡å‡†å¥½çš„æ¨¡å‹æ›´å¯é 

---

## å¿«é€Ÿå¼€å§‹

### å‰ææ¡ä»¶

1. å·²è¿è¡Œ `eval.py` ç”Ÿæˆ `detailed_predictions.csv` æ–‡ä»¶
2. CSVæ–‡ä»¶åŒ…å«å®Œæ•´çš„é¢„æµ‹æ¦‚ç‡(æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡åˆ—)

### ä½¿ç”¨æ–¹æ³•: Pythonå‡½æ•°è°ƒç”¨

```python
from tools.evaluate_calibration import evaluate_model_calibration

# ç¤ºä¾‹1: ä»CSVåŠ è½½ (æ¨è,é€Ÿåº¦å¿«)
results = evaluate_model_calibration(
    csv_path='metrics_out/inception_resnet_v2_cls_test/detailed_predictions.csv',
    output_dir='metrics_out/calibration_analysis',
    n_bins=10
)

# ç¤ºä¾‹2: å®æ—¶æ¨ç† (çµæ´»ä½†æ…¢)
from eval import Eval_Classification
model = Eval_Classification()
results = evaluate_model_calibration(
    model_instance=model,
    annotation_path='cls_test.txt',
    output_dir='metrics_out/calibration_analysis'
)

# ç¤ºä¾‹3: ä½¿ç”¨ç­‰é¢‘åˆ†æ¡¶ (é€‚åˆæ•°æ®åˆ†å¸ƒä¸å‡)
results = evaluate_model_calibration(
    csv_path='metrics_out/model/detailed_predictions.csv',
    output_dir='metrics_out/calibration',
    n_bins=10,
    binning_strategy='quantile'  # 'uniform'(é»˜è®¤) æˆ– 'quantile'
)
```

---

## å‡½æ•°å‚æ•°è¯´æ˜

### `evaluate_model_calibration()` å‚æ•°åˆ—è¡¨

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|-----|------|--------|------|
| `csv_path` | str | `None` | CSVæ–‡ä»¶è·¯å¾„(ä¼˜å…ˆä½¿ç”¨,æ¥è‡ªeval.py) |
| `annotation_path` | str | `'cls_test.txt'` | æµ‹è¯•æ•°æ®æ ‡æ³¨æ–‡ä»¶(å®æ—¶æ¨ç†æ—¶ä½¿ç”¨) |
| `model_instance` | object | `None` | Eval_Classificationå®ä¾‹(å®æ—¶æ¨ç†æ—¶ä½¿ç”¨) |
| `class_names` | list | `None` | ç±»åˆ«åç§°åˆ—è¡¨(è‡ªåŠ¨ä»CSVæˆ–æ¨¡å‹æ¨æ–­) |
| `output_dir` | str | `'metrics_out/calibration_analysis'` | è¾“å‡ºç›®å½•è·¯å¾„ |
| `n_bins` | int | `10` | åˆ†æ¡¶æ•°é‡(å»ºè®®5-20) |
| `binning_strategy` | str | `'uniform'` | åˆ†æ¡¶ç­–ç•¥: 'uniform'(ç­‰å®½) æˆ– 'quantile'(ç­‰é¢‘) |
| `verbose` | bool | `True` | æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯ |

### è¿”å›å€¼è¯´æ˜

è¿”å›ä¸€ä¸ªå­—å…¸,åŒ…å«ä»¥ä¸‹å­—æ®µ:

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `overall_brier_score` | float | æ•´ä½“Brier Score |
| `per_class_brier_scores` | list | å„ç±»åˆ«Brier Scoreåˆ—è¡¨ |
| `overall_calibration` | dict | æ•´ä½“æ ¡å‡†æ›²çº¿æ•°æ® |
| `per_class_calibrations` | list | å„ç±»åˆ«æ ¡å‡†æ›²çº¿æ•°æ®åˆ—è¡¨ |

---

## è¾“å‡ºæ–‡ä»¶

è¿è¡Œå®Œæˆå,åœ¨æŒ‡å®šçš„`output_dir`ç›®å½•ä¸‹ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶:

### 1. å¯è§†åŒ–æ–‡ä»¶

#### `calibration_overall.png`
- **å†…å®¹**: æ•´ä½“æ ¡å‡†æ›²çº¿(Overall Calibration Curve)
- **ç‰¹ç‚¹**:
  - ä½¿ç”¨çº¿å›¾è¿æ¥å„æ ¡å‡†ç‚¹(marker='o')
  - å¯¹è§’è™šçº¿è¡¨ç¤ºå®Œç¾æ ¡å‡†(Perfect calibration)
  - æ ‡é¢˜æ˜¾ç¤ºæ•´ä½“Brier Score
  - è‹±æ–‡æ ‡ç­¾,Times New Romanå­—ä½“
- **è§£è¯»**:
  - æ›²çº¿è¶Šæ¥è¿‘å¯¹è§’çº¿,æ ¡å‡†è¶Šå¥½
  - æ›²çº¿åœ¨å¯¹è§’çº¿ä¸‹æ–¹: æ¨¡å‹è¿‡äºè‡ªä¿¡(æ¦‚ç‡åé«˜)
  - æ›²çº¿åœ¨å¯¹è§’çº¿ä¸Šæ–¹: æ¨¡å‹è¿‡äºä¿å®ˆ(æ¦‚ç‡åä½)

#### `calibration_per_class.png`
- **å†…å®¹**: å„ç±»åˆ«æ ¡å‡†æ›²çº¿(Per-Class Calibration Curves)
- **å¸ƒå±€**: æ ¹æ®ç±»åˆ«æ•°é‡è‡ªåŠ¨è°ƒæ•´
  - 2åˆ†ç±»: 1è¡Œ2åˆ—
  - 3åˆ†ç±»: 1è¡Œ3åˆ—
  - 4åˆ†ç±»: 2è¡Œ2åˆ—
  - 5-6åˆ†ç±»: 2è¡Œ3åˆ—
  - 7-9åˆ†ç±»: 3è¡Œ3åˆ—
- **ç‰¹ç‚¹**:
  - æ¯ä¸ªå­å›¾æ˜¾ç¤ºä¸€ä¸ªç±»åˆ«çš„æ ¡å‡†æ›²çº¿(One-vs-Restç­–ç•¥)
  - æ ‡é¢˜æ˜¾ç¤ºç±»åˆ«åå’Œè¯¥ç±»åˆ«çš„Brier Score
  - è‹±æ–‡æ ‡ç­¾,Times New Romanå­—ä½“

### 2. æ–‡æœ¬æŠ¥å‘Š

#### `calibration_report.txt`
- **å†…å®¹**: å®Œæ•´çš„æ ¡å‡†æ€§èƒ½è¯„ä¼°æŠ¥å‘Š(ä¸­æ–‡)
- **åŒ…å«**:
  - I. æ•´ä½“æ ¡å‡†æŒ‡æ ‡(æ•´ä½“Brier Scoreã€åˆ†æ¡¶ç»Ÿè®¡)
  - II. å„ç±»åˆ«æ ¡å‡†æŒ‡æ ‡(å„ç±»åˆ«Brier Scoreã€å®å¹³å‡)
  - III. æ ¡å‡†è´¨é‡ç»¼åˆè¯„ä¼°(ä¼˜ç§€/è‰¯å¥½/ä¸€èˆ¬/è¾ƒå·®)
  - IV. Brier Scoreè§£é‡ŠæŒ‡å—(é˜ˆå€¼è¯´æ˜)
  - æŠ¥å‘Šç”Ÿæˆæ—¶é—´

---

## è¾“å‡ºè§£è¯»

### æ§åˆ¶å°è¾“å‡ºç¤ºä¾‹

```
================================================================================
æ¨¡å‹æ ¡å‡†æ€§èƒ½è¯„ä¼°
================================================================================

[1/5] åŠ è½½é¢„æµ‹æ•°æ®...
  âœ“ åŠ è½½å®Œæˆ: 500ä¸ªæ ·æœ¬, 2ä¸ªç±»åˆ«
  âœ“ ç±»åˆ«åç§°: ['normal', 'abnormal']

[2/5] è®¡ç®—Brier Score...
  âœ“ æ•´ä½“Brier Score: 0.0987
  âœ“ ç±»åˆ«0 (normal) Brier Score: 0.0823
  âœ“ ç±»åˆ«1 (abnormal) Brier Score: 0.1151

[3/5] è®¡ç®—æ ¡å‡†æ›²çº¿...
  âœ“ æ ¡å‡†æ›²çº¿è®¡ç®—å®Œæˆ (åˆ†æ¡¶æ•°: 10, ç­–ç•¥: uniform)

[4/5] ç”Ÿæˆå¯è§†åŒ–...
  âœ“ æ•´ä½“æ ¡å‡†å›¾å·²ä¿å­˜: metrics_out/calibration/calibration_overall.png
  âœ“ å„ç±»åˆ«æ ¡å‡†å›¾å·²ä¿å­˜: metrics_out/calibration/calibration_per_class.png

[5/5] ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š...
  âœ“ æ–‡æœ¬æŠ¥å‘Šå·²ä¿å­˜: metrics_out/calibration/calibration_report.txt

================================================================================
è¯„ä¼°å®Œæˆ!
================================================================================

æ•´ä½“Brier Score: 0.0987
æ•´ä½“è¯„ä»·: ä¼˜ç§€ - æ¦‚ç‡é¢„æµ‹é«˜åº¦å¯é 

å„ç±»åˆ«Brier Score:
  â€¢ ç±»åˆ«0 (normal): 0.0823 (ä¼˜ç§€)
  â€¢ ç±»åˆ«1 (abnormal): 0.1151 (è‰¯å¥½)

è¾“å‡ºæ–‡ä»¶:
  1. metrics_out/calibration/calibration_overall.png
  2. metrics_out/calibration/calibration_per_class.png
  3. metrics_out/calibration/calibration_report.txt
```

### Brier Scoreè§£è¯»

#### 2åˆ†ç±»åœºæ™¯
| Brier Score | ç­‰çº§ | è¯´æ˜ |
|-------------|------|------|
| **BS < 0.10** | ä¼˜ç§€ | æ¦‚ç‡é¢„æµ‹é«˜åº¦å¯é ,å¯ç›´æ¥ç”¨äºå†³ç­– |
| **0.10 â‰¤ BS < 0.15** | è‰¯å¥½ | æ¨¡å‹é¢„æµ‹æ¦‚ç‡å…·æœ‰ä¸­ç­‰å¯é æ€§ |
| **0.15 â‰¤ BS < 0.20** | ä¸€èˆ¬ | å­˜åœ¨ä¸€å®šç¨‹åº¦çš„æ ¡å‡†åå·®,å»ºè®®é‡æ–°æ ¡å‡† |
| **BS â‰¥ 0.20** | è¾ƒå·® | æ˜¾è‘—çš„æ ¡å‡†åå·®,éœ€è¦é‡æ–°æ ¡å‡† |

**å‚è€ƒåŸºå‡†**: éšæœºåˆ†ç±»å™¨çš„Brier Score â‰ˆ 0.25

#### å¤šåˆ†ç±»åœºæ™¯ (Nâ‰¥3)
- **é˜ˆå€¼åŠ¨æ€è°ƒæ•´**: éšæœºåˆ†ç±»å™¨åŸºå‡† BS = 1 - (1/C)
- **ä¼˜ç§€**: BS < baseline Ã— 0.4
- **è‰¯å¥½**: baseline Ã— 0.4 â‰¤ BS < baseline Ã— 0.6
- **ä¸€èˆ¬**: baseline Ã— 0.6 â‰¤ BS < baseline Ã— 0.8
- **è¾ƒå·®**: BS â‰¥ baseline Ã— 0.8

---

## ä½¿ç”¨æ¡ˆä¾‹

### æ¡ˆä¾‹1: è¯„ä¼°å•ä¸ªæ¨¡å‹çš„æ ¡å‡†æ€§èƒ½

```python
# åœºæ™¯: è®­ç»ƒå®Œæˆå,è¯„ä¼°æ¨¡å‹çš„æ¦‚ç‡é¢„æµ‹æ˜¯å¦å¯é 
from tools.evaluate_calibration import evaluate_model_calibration

results = evaluate_model_calibration(
    csv_path='metrics_out/inception_resnet_v2_cls_test/detailed_predictions.csv',
    output_dir='metrics_out/inception_resnet_v2_calibration'
)

# åˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ–°æ ¡å‡†
if results['overall_brier_score'] < 0.10:
    print("âœ“ æ¨¡å‹æ ¡å‡†è‰¯å¥½,å¯ç›´æ¥éƒ¨ç½²")
else:
    print("âš  å»ºè®®åº”ç”¨æ ¡å‡†æ–¹æ³•(å¦‚Platt Scaling)")
```

**é¢„æœŸè¾“å‡º**: åˆ¤æ–­æ¨¡å‹æ˜¯å¦é€‚åˆç”¨äºæ¦‚ç‡é˜ˆå€¼å†³ç­–

### æ¡ˆä¾‹2: å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„æ ¡å‡†æ€§èƒ½

```python
# åœºæ™¯: æ¯”è¾ƒæ¨¡å‹Aå’Œæ¨¡å‹Bå“ªä¸ªæ ¡å‡†æ›´å¥½
from tools.evaluate_calibration import evaluate_model_calibration

# è¯„ä¼°æ¨¡å‹A
results_A = evaluate_model_calibration(
    csv_path='metrics_out/model_A/detailed_predictions.csv',
    output_dir='metrics_out/calibration_A',
    verbose=False
)

# è¯„ä¼°æ¨¡å‹B
results_B = evaluate_model_calibration(
    csv_path='metrics_out/model_B/detailed_predictions.csv',
    output_dir='metrics_out/calibration_B',
    verbose=False
)

# å¯¹æ¯”Brier Score
bs_A = results_A['overall_brier_score']
bs_B = results_B['overall_brier_score']

print(f"æ¨¡å‹A Brier Score: {bs_A:.4f}")
print(f"æ¨¡å‹B Brier Score: {bs_B:.4f}")
print(f"å·®å¼‚: {abs(bs_A - bs_B):.4f}")

if bs_A < bs_B:
    print("âœ“ æ¨¡å‹Aæ ¡å‡†æ›´å¥½")
else:
    print("âœ“ æ¨¡å‹Bæ ¡å‡†æ›´å¥½")
```

**é¢„æœŸè¾“å‡º**: è¯†åˆ«æ ¡å‡†æ€§èƒ½æ›´å¥½çš„æ¨¡å‹

### æ¡ˆä¾‹3: å®æ—¶æ¨ç†è¯„ä¼°(ä¸ä½¿ç”¨CSV)

```python
# åœºæ™¯: å¿«é€Ÿè¯„ä¼°æ–°è®­ç»ƒçš„æ¨¡å‹,æ— éœ€å…ˆè¿è¡Œeval.py
from eval import Eval_Classification
from tools.evaluate_calibration import evaluate_model_calibration

# åŠ è½½æ¨¡å‹
model = Eval_Classification()

# ç›´æ¥è¯„ä¼°
results = evaluate_model_calibration(
    model_instance=model,
    annotation_path='cls_test.txt',
    output_dir='metrics_out/quick_calibration'
)
```

**é¢„æœŸè¾“å‡º**: å¿«é€Ÿè·å–æ ¡å‡†åˆ†æç»“æœ

---

## å¸¸è§é—®é¢˜ (FAQ)

### Q1: Calibration Plotå’ŒROCæ›²çº¿æœ‰ä»€ä¹ˆåŒºåˆ«?

**A**:
- **ROCæ›²çº¿**: è¯„ä¼°æ¨¡å‹åŒºåˆ†ç±»åˆ«çš„èƒ½åŠ›(æ’åºèƒ½åŠ›)
- **Calibration Plot**: è¯„ä¼°æ¨¡å‹è¾“å‡ºæ¦‚ç‡çš„å¯é æ€§(æ¦‚ç‡å‡†ç¡®æ€§)

**ç¤ºä¾‹**:
- æ¨¡å‹A: AUC=0.95, BS=0.25 â†’ æ’åºèƒ½åŠ›å¼º,ä½†æ¦‚ç‡ä¸å¯é 
- æ¨¡å‹B: AUC=0.90, BS=0.08 â†’ æ’åºç¨å¼±,ä½†æ¦‚ç‡é«˜åº¦å¯é 

**æ¨è**: åŒæ—¶å…³æ³¨AUCå’ŒBrier Score

---

### Q2: ä»€ä¹ˆæ—¶å€™åº”è¯¥è¿›è¡Œæ¨¡å‹é‡æ–°æ ¡å‡†?

**A**: ä»¥ä¸‹æƒ…å†µå»ºè®®é‡æ–°æ ¡å‡†:
1. Brier Score â‰¥ 0.15 (2åˆ†ç±») æˆ– â‰¥ baseline Ã— 0.6 (å¤šåˆ†ç±»)
2. Calibration Plotæ›²çº¿æ˜æ˜¾åç¦»å¯¹è§’çº¿
3. éœ€è¦ä½¿ç”¨æ¦‚ç‡é˜ˆå€¼è¿›è¡Œå†³ç­–(å¦‚åŒ»å­¦è¯Šæ–­ã€é£é™©æ§åˆ¶)
4. æ¨¡å‹åœ¨æ–°æ•°æ®åˆ†å¸ƒä¸Šéƒ¨ç½²

**å¸¸ç”¨æ ¡å‡†æ–¹æ³•**:
- Platt Scaling (é€‚åˆSVMã€ç¥ç»ç½‘ç»œ)
- Isotonic Regression (æ— å‚æ•°å‡è®¾,æ›´çµæ´»)
- Temperature Scaling (æ·±åº¦å­¦ä¹ æ¨¡å‹å¸¸ç”¨)

---

### Q3: ç­‰å®½åˆ†æ¡¶(uniform)å’Œç­‰é¢‘åˆ†æ¡¶(quantile)å¦‚ä½•é€‰æ‹©?

**A**:
- **ç­‰å®½åˆ†æ¡¶** (`strategy='uniform'`):
  - å°†æ¦‚ç‡ç©ºé—´[0, 1]å‡åŒ€åˆ’åˆ†
  - é€‚åˆ: æ¦‚ç‡åˆ†å¸ƒè¾ƒå‡åŒ€çš„åœºæ™¯
  - **é»˜è®¤æ¨è**

- **ç­‰é¢‘åˆ†æ¡¶** (`strategy='quantile'`):
  - æ¯ä¸ªæ¡¶åŒ…å«ç›¸åŒæ•°é‡çš„æ ·æœ¬
  - é€‚åˆ: æ¦‚ç‡é«˜åº¦é›†ä¸­åœ¨æŸäº›åŒºé—´(å¦‚å¤§é‡é«˜ç½®ä¿¡åº¦é¢„æµ‹)
  - å¯èƒ½å‡ºç°æ¡¶è¾¹ç•Œé‡å 

**ç»éªŒ**: ä¼˜å…ˆä½¿ç”¨ç­‰å®½åˆ†æ¡¶,é™¤éå¤§é‡æ ·æœ¬é›†ä¸­åœ¨æŸäº›æ¦‚ç‡åŒºé—´

---

### Q4: åˆ†æ¡¶æ•°é‡(n_bins)å¦‚ä½•é€‰æ‹©?

**A**:
| æ ·æœ¬é‡ | æ¨èn_bins | è¯´æ˜ |
|--------|-----------|------|
| < 100 | 5 | é¿å…æ¡¶å†…æ ·æœ¬è¿‡å°‘ |
| 100-500 | 10 | **é»˜è®¤å€¼,é€‚åˆå¤§å¤šæ•°åœºæ™¯** |
| 500-1000 | 15 | æ›´ç²¾ç»†çš„æ ¡å‡†åˆ†æ |
| > 1000 | 20 | é«˜ç²¾åº¦æ ¡å‡†æ›²çº¿ |

**åŸåˆ™**: n_bins Ã— 2 â‰¤ æ ·æœ¬é‡(ç¡®ä¿æ¯ä¸ªæ¡¶æœ‰è¶³å¤Ÿæ ·æœ¬)

---

### Q5: æŠ¥é”™ "CSVæ–‡ä»¶æœªæ‰¾åˆ°æ¦‚ç‡åˆ—" æ€ä¹ˆåŠ?

**A**: æ£€æŸ¥CSVæ–‡ä»¶æ ¼å¼:

**æ­£ç¡®æ ¼å¼** (eval.pyç”Ÿæˆçš„æ ¼å¼):
```csv
path,true,predict,normal_probability,abnormal_probability
datasets/test/0/img1.jpg,0,0,0.92,0.08
```

æˆ–

```csv
path,true,predict,class_0_prob,class_1_prob
datasets/test/0/img1.jpg,0,0,0.92,0.08
```

**é”™è¯¯æ ¼å¼** (ä¸æ”¯æŒ):
- åˆ—å: `prob_0`, `probability_normal` (ä¸ç¬¦åˆå‘½åè§„èŒƒ)
- ç¼ºå°‘æ¦‚ç‡åˆ—(åªæœ‰path, true, predict)

**è§£å†³æ–¹æ³•**: é‡æ–°è¿è¡Œ `eval.py` ç”Ÿæˆæ­£ç¡®æ ¼å¼çš„CSV

---

### Q6: å¦‚ä½•è§£è¯»å„ç±»åˆ«æ ¡å‡†å›¾?

**A**: ä»¥2åˆ†ç±»ä¸ºä¾‹:

**ç±»åˆ«0 (normal) æ ¡å‡†å›¾**:
- Xè½´: æ¨¡å‹é¢„æµ‹ä¸ºç±»åˆ«0çš„æ¦‚ç‡
- Yè½´: çœŸå®ä¸ºç±»åˆ«0çš„æ ·æœ¬æ¯”ä¾‹
- **å®Œç¾æ ¡å‡†**: æ¦‚ç‡0.8 â†’ çœŸå®æ¯”ä¾‹0.8

**ç±»åˆ«1 (abnormal) æ ¡å‡†å›¾**:
- Xè½´: æ¨¡å‹é¢„æµ‹ä¸ºç±»åˆ«1çš„æ¦‚ç‡
- Yè½´: çœŸå®ä¸ºç±»åˆ«1çš„æ ·æœ¬æ¯”ä¾‹

**å¸¸è§é—®é¢˜**:
- æŸç±»åˆ«æ ¡å‡†å·®,å…¶ä»–ç±»åˆ«å¥½ â†’ å¯èƒ½æ•°æ®ä¸å¹³è¡¡å¯¼è‡´
- æ‰€æœ‰ç±»åˆ«éƒ½æ ¡å‡†å·® â†’ å»ºè®®é‡æ–°è®­ç»ƒæˆ–åº”ç”¨å…¨å±€æ ¡å‡†æ–¹æ³•

---

### Q7: Brier Scoreå’ŒECE(Expected Calibration Error)æœ‰ä»€ä¹ˆåŒºåˆ«?

**A**:
- **Brier Score**:
  - æ¦‚ç‡é¢„æµ‹çš„å‡æ–¹è¯¯å·®
  - åŒæ—¶è€ƒè™‘æ ¡å‡†å’Œåˆ†è¾¨ç‡
  - sklearnæ ‡å‡†APIæ”¯æŒ

- **ECE** (æœ¬å·¥å…·æœªå®ç°):
  - ä»…è¡¡é‡æ ¡å‡†åå·®
  - æ›´ç›´è§‚,ä½†éœ€è¦æ‰‹åŠ¨å®ç°

**æœ¬å·¥å…·é€‰æ‹©**: Brier Scoreå› å…¶æ ‡å‡†åŒ–å’Œå¹¿æ³›è®¤å¯è€Œè¢«é‡‡ç”¨

---

### Q8: èƒ½å¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç›‘æ§æ ¡å‡†æ€§èƒ½?

**A**: å¯ä»¥,ä¿®æ”¹`train_trimm.py`æ·»åŠ éªŒè¯é›†æ ¡å‡†ç›‘æ§:

```python
# åœ¨éªŒè¯é˜¶æ®µæ·»åŠ 
from tools.evaluate_calibration import compute_brier_score_multiclass

def validate_epoch(model, val_loader):
    # æ”¶é›†é¢„æµ‹æ¦‚ç‡å’ŒçœŸå®æ ‡ç­¾
    all_probs = []
    all_labels = []

    for images, labels in val_loader:
        probs = model(images)
        all_probs.append(probs.cpu().numpy())
        all_labels.append(labels.cpu().numpy())

    # è®¡ç®—Brier Score
    probs = np.concatenate(all_probs)
    labels = np.concatenate(all_labels)
    bs = compute_brier_score_multiclass(labels, probs)

    print(f"Validation Brier Score: {bs:.4f}")
    return bs
```

---

### Q9: æ¨¡å‹æ ¡å‡†å¥½ä½†AUCä½,è¯¥å¦‚ä½•æ”¹è¿›?

**A**:
- **åˆ†æ**: æ¨¡å‹çš„æ¦‚ç‡è¾“å‡ºå¯é ,ä½†åŒºåˆ†èƒ½åŠ›ä¸è¶³
- **æ”¹è¿›æ–¹å‘**:
  1. å¢å¼ºç‰¹å¾æå–(æ›´æ¢æ¨¡å‹æ¶æ„)
  2. æ•°æ®å¢å¼ºæˆ–æ”¶é›†æ›´å¤šæ•°æ®
  3. è°ƒæ•´æŸå¤±å‡½æ•°(å¦‚Focal Loss)
  4. è¶…å‚æ•°è°ƒä¼˜

**æ³¨æ„**: æ ¡å‡†å¥½æ˜¯å‰æ,å…ˆæå‡AUC,å†è°ƒæ•´æ ¡å‡†

---

### Q10: å¦‚ä½•å°†æ ¡å‡†ç»“æœç”¨äºè®ºæ–‡?

**A**: æ¨èå†…å®¹:

**1. è¡¨æ ¼**: åœ¨è®ºæ–‡ä¸­æ·»åŠ æ¨¡å‹æ€§èƒ½å¯¹æ¯”è¡¨
```
| æ¨¡å‹ | AUC | Brier Score | æ ¡å‡†è´¨é‡ |
|------|-----|-------------|---------|
| æ¨¡å‹A | 0.95 | 0.089 | ä¼˜ç§€ |
| æ¨¡å‹B | 0.93 | 0.145 | è‰¯å¥½ |
```

**2. å›¾è¡¨**: ä½¿ç”¨ç”Ÿæˆçš„`calibration_overall.png`
- æ ‡é¢˜: "Model Calibration Performance"
- è¯´æ˜: "The calibration curve shows the relationship between predicted probabilities and actual frequencies. Closer alignment with the diagonal indicates better calibration."

**3. æ–‡æœ¬æè¿°**:
"æ¨¡å‹çš„æ•´ä½“Brier Scoreä¸º0.089,è¡¨æ˜æ¦‚ç‡é¢„æµ‹é«˜åº¦å¯é ã€‚æ ¡å‡†æ›²çº¿(å›¾X)æ˜¾ç¤ºé¢„æµ‹æ¦‚ç‡ä¸å®é™…æ­£ç¡®ç‡é«˜åº¦å»åˆ,é€‚åˆç”¨äºä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿã€‚"

---

## æŠ€æœ¯ç»†èŠ‚

### æ ¡å‡†æ›²çº¿è®¡ç®—æ–¹æ³•

æœ¬å·¥å…·ä½¿ç”¨`sklearn.calibration.calibration_curve` API:

#### æ•´ä½“æ ¡å‡†æ›²çº¿
```python
# æ­¥éª¤1: æå–æœ€å¤§æ¦‚ç‡å’Œæ­£ç¡®æ€§
max_probs = np.max(probs, axis=1)
predicted_labels = np.argmax(probs, axis=1)
correct = (predicted_labels == labels).astype(int)

# æ­¥éª¤2: ä½¿ç”¨sklearn API
true_frequencies, mean_predicted_probs = calibration_curve(
    y_true=correct,
    y_prob=max_probs,
    n_bins=10,
    strategy='uniform'
)
```

#### å„ç±»åˆ«æ ¡å‡†æ›²çº¿ (One-vs-Rest)
```python
# æ­¥éª¤1: äºŒå€¼åŒ–
binary_labels = (labels == class_idx).astype(int)
class_probs = probs[:, class_idx]

# æ­¥éª¤2: ä½¿ç”¨sklearn API
true_frequencies, mean_predicted_probs = calibration_curve(
    y_true=binary_labels,
    y_prob=class_probs,
    n_bins=10,
    strategy='uniform'
)
```

### Brier Scoreè®¡ç®—å…¬å¼

#### å¤šåˆ†ç±»Brier Score (æ•´ä½“)
```
BS = (1/N) * Î£_{i=1}^{N} Î£_{j=1}^{C} (p_{ij} - y_{ij})^2

å…¶ä¸­:
  N = æ ·æœ¬æ•°é‡
  C = ç±»åˆ«æ•°é‡
  p_{ij} = æ ·æœ¬ié¢„æµ‹ä¸ºç±»åˆ«jçš„æ¦‚ç‡
  y_{ij} = æ ·æœ¬içœŸå®æ ‡ç­¾çš„one-hotç¼–ç 
```

#### å•ç±»åˆ«Brier Score (One-vs-Rest)
```
BS_k = (1/N) * Î£_{i=1}^{N} (p_{ik} - y_{ik})^2

å…¶ä¸­:
  p_{ik} = æ ·æœ¬ié¢„æµ‹ä¸ºç±»åˆ«kçš„æ¦‚ç‡
  y_{ik} = æ ·æœ¬iæ˜¯å¦ä¸ºç±»åˆ«k (0æˆ–1)
```

### é€‚ç”¨åœºæ™¯

âœ… **é€‚ç”¨**:
- éœ€è¦ä½¿ç”¨æ¦‚ç‡é˜ˆå€¼è¿›è¡Œå†³ç­–çš„ä»»åŠ¡
- åŒ»å­¦è¯Šæ–­ã€é£é™©è¯„ä¼°ç­‰é«˜é£é™©åœºæ™¯
- æ¨¡å‹éƒ¨ç½²å‰çš„æœ€ç»ˆéªŒè¯
- å¤šæ¨¡å‹é€‰æ‹©æ—¶çš„è¾…åŠ©æŒ‡æ ‡

âŒ **ä¸é€‚ç”¨**:
- ä»…å…³å¿ƒåˆ†ç±»å‡†ç¡®ç‡,ä¸ä½¿ç”¨æ¦‚ç‡å€¼
- æ ·æœ¬é‡è¿‡å°(<50æ ·æœ¬,åˆ†æ¡¶ä¸ç¨³å®š)
- åªéœ€è¦æ’åºèƒ½åŠ›(å¦‚æ¨èç³»ç»Ÿ,ä½¿ç”¨AUCå³å¯)


## æ›´æ–°æ—¥å¿—

### v1.0.0 (2025-11-28)
- âœ… åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- âœ… æ”¯æŒæ•´ä½“å’Œå„ç±»åˆ«æ ¡å‡†åˆ†æ
- âœ… åŸºäºsklearn.calibration.calibration_curveå®ç°
- âœ… Brier Scoreè®¡ç®—(æ•´ä½“+å„ç±»åˆ«)
- âœ… ä¸“ä¸šå¯è§†åŒ–(è‹±æ–‡+Times New Romanå­—ä½“)
- âœ… å®Œæ•´æ–‡æœ¬æŠ¥å‘Šç”Ÿæˆ(ä¸­æ–‡)
- âœ… æ”¯æŒCSVåŠ è½½å’Œå®æ—¶æ¨ç†ä¸¤ç§æ¨¡å¼
- âœ… ç­‰å®½/ç­‰é¢‘åˆ†æ¡¶ç­–ç•¥
- âœ… çº¯Pythonå‡½æ•°æ¥å£,æ— å‘½ä»¤è¡Œä¾èµ–

---


**Happy Calibrating! ğŸ“Š**
